[
  {
    "run_id": "20251214T025815Z",
    "created_at": "2025-12-14T02:58:21.278747+00:00",
    "version": "kjv",
    "prompt_mode": "system2",
    "sample": {
      "count": 10,
      "seed": 1,
      "stratified": false
    },
    "models": [
      {
        "model": "reference:verbatim",
        "model_slug": "reference_verbatim",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 1.0,
        "avg_wer": 0.0,
        "avg_cer": 0.0,
        "avg_token_sort_ratio": 100.0,
        "avg_chatter_ratio": 0.0,
        "results_path": "runs/20251214T025815Z/reference_verbatim/results.json"
      },
      {
        "model": "ollama:llama3.2",
        "model_slug": "ollama_llama3_2",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.2,
        "avg_wer": 0.635694127980573,
        "avg_cer": 0.43271801702238033,
        "avg_token_sort_ratio": 66.77451135950984,
        "avg_chatter_ratio": -0.10976615420649527,
        "results_path": "runs/20251214T025815Z/ollama_llama3_2/results.json"
      }
    ]
  },
  {
    "run_id": "20251214T030555Z",
    "created_at": "2025-12-14T03:06:00.506994+00:00",
    "version": "kjv",
    "prompt_mode": "system2",
    "sample": {
      "count": 10,
      "seed": 1,
      "stratified": false
    },
    "models": [
      {
        "model": "reference:verbatim",
        "model_slug": "reference_verbatim",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 1.0,
        "avg_wer": 0.0,
        "avg_cer": 0.0,
        "avg_token_sort_ratio": 100.0,
        "avg_chatter_ratio": 0.0,
        "avg_len_ratio": 1.0,
        "verbatim_count": 10,
        "inaccurate_count": 0,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 0,
        "grade": "A",
        "headline": "Excellent verbatim recall.",
        "notes": [
          "Behaves consistently on this sample."
        ],
        "results_path": "runs/20251214T030555Z/reference_verbatim/results.json",
        "details_path": "results/details/20251214T030555Z/reference_verbatim.json",
        "details_rel": "details/20251214T030555Z/reference_verbatim.json"
      },
      {
        "model": "ollama:llama3.2",
        "model_slug": "ollama_llama3_2",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.2,
        "avg_wer": 0.635694127980573,
        "avg_cer": 0.43271801702238033,
        "avg_token_sort_ratio": 66.77451135950984,
        "avg_chatter_ratio": -0.10976615420649527,
        "avg_len_ratio": 0.8902338457935047,
        "verbatim_count": 2,
        "inaccurate_count": 8,
        "hallucination_count": 0,
        "chattery_count": 1,
        "truncated_count": 4,
        "grade": "F",
        "headline": "Low verbatim fidelity.",
        "notes": [
          "Often truncates output."
        ],
        "results_path": "runs/20251214T030555Z/ollama_llama3_2/results.json",
        "details_path": "results/details/20251214T030555Z/ollama_llama3_2.json",
        "details_rel": "details/20251214T030555Z/ollama_llama3_2.json"
      }
    ]
  },
  {
    "run_id": "20251214T031246Z",
    "created_at": "2025-12-14T03:12:50.949735+00:00",
    "version": "kjv",
    "prompt_mode": "system2",
    "sample": {
      "count": 10,
      "seed": 1,
      "stratified": false
    },
    "models": [
      {
        "model": "reference:verbatim",
        "model_slug": "reference_verbatim",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 1.0,
        "avg_wer": 0.0,
        "avg_cer": 0.0,
        "avg_token_sort_ratio": 100.0,
        "avg_chatter_ratio": 0.0,
        "avg_len_ratio": 1.0,
        "verbatim_count": 10,
        "inaccurate_count": 0,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 0,
        "grade": "A",
        "headline": "Excellent verbatim recall.",
        "notes": [
          "Behaves consistently on this sample."
        ],
        "results_path": "runs/20251214T031246Z/reference_verbatim/results.json",
        "details_path": "results/details/20251214T031246Z/reference_verbatim.json",
        "details_rel": "details/20251214T031246Z/reference_verbatim.json"
      },
      {
        "model": "ollama:llama3.2",
        "model_slug": "ollama_llama3_2",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.2,
        "avg_wer": 0.635694127980573,
        "avg_cer": 0.43271801702238033,
        "avg_token_sort_ratio": 66.77451135950984,
        "avg_chatter_ratio": -0.10976615420649527,
        "avg_len_ratio": 0.8902338457935047,
        "verbatim_count": 2,
        "inaccurate_count": 8,
        "hallucination_count": 0,
        "chattery_count": 1,
        "truncated_count": 4,
        "grade": "F",
        "headline": "Low verbatim fidelity.",
        "notes": [
          "Often truncates output."
        ],
        "results_path": "runs/20251214T031246Z/ollama_llama3_2/results.json",
        "details_path": "results/details/20251214T031246Z/ollama_llama3_2.json",
        "details_rel": "details/20251214T031246Z/ollama_llama3_2.json"
      }
    ]
  },
  {
    "run_id": "20251214T032219Z",
    "created_at": "2025-12-14T03:22:22.830711+00:00",
    "version": "kjv",
    "prompt_mode": "system2",
    "sample": {
      "count": 10,
      "seed": 1,
      "stratified": false
    },
    "models": [
      {
        "model": "reference:verbatim",
        "model_slug": "reference_verbatim",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 1.0,
        "avg_wer": 0.0,
        "avg_cer": 0.0,
        "avg_token_sort_ratio": 100.0,
        "avg_chatter_ratio": 0.0,
        "avg_len_ratio": 1.0,
        "verbatim_count": 10,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 0,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 0,
        "content_accuracy": 1.0,
        "grade": "A",
        "headline": "Excellent verbatim recall.",
        "notes": [
          "Behaves consistently on this sample."
        ],
        "results_path": "runs/20251214T032219Z/reference_verbatim/results.json",
        "details_path": "results/details/20251214T032219Z/reference_verbatim.json",
        "details_rel": "details/20251214T032219Z/reference_verbatim.json"
      },
      {
        "model": "ollama:llama3.2",
        "model_slug": "ollama_llama3_2",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.2,
        "avg_wer": 0.635694127980573,
        "avg_cer": 0.43271801702238033,
        "avg_token_sort_ratio": 66.77451135950984,
        "avg_chatter_ratio": -0.10976615420649527,
        "avg_len_ratio": 0.8902338457935047,
        "verbatim_count": 2,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 8,
        "hallucination_count": 0,
        "chattery_count": 1,
        "truncated_count": 4,
        "content_accuracy": 0.2,
        "grade": "F",
        "headline": "Low verbatim fidelity.",
        "notes": [
          "Often truncates output."
        ],
        "results_path": "runs/20251214T032219Z/ollama_llama3_2/results.json",
        "details_path": "results/details/20251214T032219Z/ollama_llama3_2.json",
        "details_rel": "details/20251214T032219Z/ollama_llama3_2.json"
      }
    ]
  },
  {
    "run_id": "20251214T032322Z",
    "created_at": "2025-12-14T03:23:25.792803+00:00",
    "version": "kjv",
    "prompt_mode": "system2",
    "sample": {
      "count": 10,
      "seed": 1,
      "stratified": false
    },
    "models": [
      {
        "model": "reference:verbatim",
        "model_slug": "reference_verbatim",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 1.0,
        "avg_wer": 0.0,
        "avg_cer": 0.0,
        "avg_token_sort_ratio": 100.0,
        "avg_chatter_ratio": 0.0,
        "avg_len_ratio": 1.0,
        "verbatim_count": 10,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 0,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 0,
        "content_accuracy": 1.0,
        "grade": "A",
        "headline": "Excellent verbatim recall.",
        "notes": [
          "Behaves consistently on this sample."
        ],
        "results_path": "runs/20251214T032322Z/reference_verbatim/results.json",
        "details_path": "results/details/20251214T032322Z/reference_verbatim.json",
        "details_rel": "details/20251214T032322Z/reference_verbatim.json"
      },
      {
        "model": "ollama:llama3.2",
        "model_slug": "ollama_llama3_2",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.2,
        "avg_wer": 0.635694127980573,
        "avg_cer": 0.43271801702238033,
        "avg_token_sort_ratio": 66.77451135950984,
        "avg_chatter_ratio": -0.10976615420649527,
        "avg_len_ratio": 0.8902338457935047,
        "verbatim_count": 2,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 8,
        "hallucination_count": 0,
        "chattery_count": 1,
        "truncated_count": 4,
        "content_accuracy": 0.2,
        "grade": "F",
        "headline": "Low verbatim fidelity.",
        "notes": [
          "Often truncates output."
        ],
        "results_path": "runs/20251214T032322Z/ollama_llama3_2/results.json",
        "details_path": "results/details/20251214T032322Z/ollama_llama3_2.json",
        "details_rel": "details/20251214T032322Z/ollama_llama3_2.json"
      }
    ]
  },
  {
    "run_id": "20251214T032850Z",
    "created_at": "2025-12-14T03:28:53.854627+00:00",
    "version": "kjv",
    "prompt_mode": "system2",
    "sample": {
      "count": 10,
      "seed": 1,
      "stratified": false
    },
    "models": [
      {
        "model": "reference:verbatim",
        "model_slug": "reference_verbatim",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 1.0,
        "avg_wer": 0.0,
        "avg_cer": 0.0,
        "avg_token_sort_ratio": 100.0,
        "avg_chatter_ratio": 0.0,
        "avg_len_ratio": 1.0,
        "verbatim_count": 10,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 0,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 0,
        "content_accuracy": 1.0,
        "grade": "A",
        "headline": "Excellent verbatim recall.",
        "notes": [
          "Behaves consistently on this sample."
        ],
        "results_path": "runs/20251214T032850Z/reference_verbatim/results.json",
        "details_path": "results/details/20251214T032850Z/reference_verbatim.json",
        "details_rel": "details/20251214T032850Z/reference_verbatim.json"
      },
      {
        "model": "ollama:llama3.2",
        "model_slug": "ollama_llama3_2",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.3,
        "avg_wer": 0.37810387765119224,
        "avg_cer": 0.27261782965270215,
        "avg_token_sort_ratio": 78.1494506631753,
        "avg_chatter_ratio": -0.1704044579533941,
        "avg_len_ratio": 0.8295955420466059,
        "verbatim_count": 3,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 7,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 4,
        "content_accuracy": 0.3,
        "grade": "D",
        "headline": "Mostly paraphrase/partial recall.",
        "notes": [
          "Often truncates output."
        ],
        "results_path": "runs/20251214T032850Z/ollama_llama3_2/results.json",
        "details_path": "results/details/20251214T032850Z/ollama_llama3_2.json",
        "details_rel": "details/20251214T032850Z/ollama_llama3_2.json"
      }
    ]
  },
  {
    "run_id": "20251214T033157Z",
    "created_at": "2025-12-14T03:32:00.901036+00:00",
    "version": "kjv",
    "prompt_mode": "system2",
    "sample": {
      "count": 10,
      "seed": 1,
      "stratified": false
    },
    "models": [
      {
        "model": "reference:verbatim",
        "model_slug": "reference_verbatim",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 1.0,
        "avg_wer": 0.0,
        "avg_cer": 0.0,
        "avg_token_sort_ratio": 100.0,
        "avg_chatter_ratio": 0.0,
        "avg_len_ratio": 1.0,
        "verbatim_count": 10,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 0,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 0,
        "content_accuracy": 1.0,
        "grade": "A",
        "headline": "Excellent verbatim recall.",
        "notes": [
          "Behaves consistently on this sample."
        ],
        "results_path": "runs/20251214T033157Z/reference_verbatim/results.json",
        "details_path": "results/details/20251214T033157Z/reference_verbatim.json",
        "details_rel": "details/20251214T033157Z/reference_verbatim.json"
      },
      {
        "model": "ollama:llama3.2",
        "model_slug": "ollama_llama3_2",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.3,
        "avg_wer": 0.37810387765119224,
        "avg_cer": 0.27261782965270215,
        "avg_token_sort_ratio": 78.1494506631753,
        "avg_chatter_ratio": -0.1704044579533941,
        "avg_len_ratio": 0.8295955420466059,
        "verbatim_count": 3,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 7,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 4,
        "content_accuracy": 0.3,
        "grade": "D",
        "headline": "Mostly paraphrase/partial recall.",
        "notes": [
          "Often truncates output."
        ],
        "results_path": "runs/20251214T033157Z/ollama_llama3_2/results.json",
        "details_path": "results/details/20251214T033157Z/ollama_llama3_2.json",
        "details_rel": "details/20251214T033157Z/ollama_llama3_2.json"
      }
    ]
  },
  {
    "run_id": "20251214T033619Z",
    "created_at": "2025-12-14T03:37:26.854371+00:00",
    "version": "kjv",
    "prompt_mode": "system2",
    "sample": {
      "count": 10,
      "seed": 1,
      "stratified": false
    },
    "models": [
      {
        "model": "reference:verbatim",
        "model_slug": "reference_verbatim",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 1.0,
        "avg_wer": 0.0,
        "avg_cer": 0.0,
        "avg_token_sort_ratio": 100.0,
        "avg_chatter_ratio": 0.0,
        "avg_len_ratio": 1.0,
        "verbatim_count": 10,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 0,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 0,
        "content_accuracy": 1.0,
        "grade": "A",
        "headline": "Excellent verbatim recall.",
        "notes": [
          "Behaves consistently on this sample."
        ],
        "results_path": "runs/20251214T033619Z/reference_verbatim/results.json",
        "details_path": "results/details/20251214T033619Z/reference_verbatim.json",
        "details_rel": "details/20251214T033619Z/reference_verbatim.json"
      },
      {
        "model": "ollama:llama3.2",
        "model_slug": "ollama_llama3_2",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.3,
        "avg_wer": 0.37810387765119224,
        "avg_cer": 0.27261782965270215,
        "avg_token_sort_ratio": 78.1494506631753,
        "avg_chatter_ratio": -0.1704044579533941,
        "avg_len_ratio": 0.8295955420466059,
        "verbatim_count": 3,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 7,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 4,
        "content_accuracy": 0.3,
        "grade": "D",
        "headline": "Mostly paraphrase/partial recall.",
        "notes": [
          "Often truncates output."
        ],
        "results_path": "runs/20251214T033619Z/ollama_llama3_2/results.json",
        "details_path": "results/details/20251214T033619Z/ollama_llama3_2.json",
        "details_rel": "details/20251214T033619Z/ollama_llama3_2.json"
      },
      {
        "model": "ollama:deepseek-r1:8b",
        "model_slug": "ollama_deepseek_r1_8b",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.0,
        "avg_wer": 22.411307842513683,
        "avg_cer": 23.864731719696557,
        "avg_token_sort_ratio": 1.142610080180663,
        "avg_chatter_ratio": 23.801864405482114,
        "avg_len_ratio": 24.801864405482114,
        "verbatim_count": 0,
        "verbatim_with_extras_count": 2,
        "inaccurate_count": 0,
        "hallucination_count": 8,
        "chattery_count": 10,
        "truncated_count": 0,
        "content_accuracy": 0.2,
        "grade": "F",
        "headline": "Low verbatim fidelity.",
        "notes": [
          "Frequently off-target (hallucinations).",
          "Often adds quotes/citations around correct text.",
          "Often adds extra chatter."
        ],
        "results_path": "runs/20251214T033619Z/ollama_deepseek_r1_8b/results.json",
        "details_path": "results/details/20251214T033619Z/ollama_deepseek_r1_8b.json",
        "details_rel": "details/20251214T033619Z/ollama_deepseek_r1_8b.json"
      }
    ]
  },
  {
    "run_id": "20251214T034359Z",
    "created_at": "2025-12-14T03:45:03.227136+00:00",
    "version": "kjv",
    "prompt_mode": "system2",
    "sample": {
      "count": 10,
      "seed": 1,
      "stratified": false
    },
    "models": [
      {
        "model": "reference:verbatim",
        "model_slug": "reference_verbatim",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 1.0,
        "avg_wer": 0.0,
        "avg_cer": 0.0,
        "avg_token_sort_ratio": 100.0,
        "avg_chatter_ratio": 0.0,
        "avg_len_ratio": 1.0,
        "verbatim_count": 10,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 0,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 0,
        "content_accuracy": 1.0,
        "grade": "A",
        "headline": "Excellent verbatim recall.",
        "notes": [
          "Behaves consistently on this sample."
        ],
        "results_path": "runs/20251214T034359Z/reference_verbatim/results.json",
        "details_path": "results/details/20251214T034359Z/reference_verbatim.json",
        "details_rel": "details/20251214T034359Z/reference_verbatim.json"
      },
      {
        "model": "ollama:llama3.2",
        "model_slug": "ollama_llama3_2",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.3,
        "avg_wer": 0.37810387765119224,
        "avg_cer": 0.27261782965270215,
        "avg_token_sort_ratio": 78.1494506631753,
        "avg_chatter_ratio": -0.1704044579533941,
        "avg_len_ratio": 0.8295955420466059,
        "verbatim_count": 3,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 7,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 4,
        "content_accuracy": 0.3,
        "grade": "D",
        "headline": "Mostly paraphrase/partial recall.",
        "notes": [
          "Often truncates output."
        ],
        "results_path": "runs/20251214T034359Z/ollama_llama3_2/results.json",
        "details_path": "results/details/20251214T034359Z/ollama_llama3_2.json",
        "details_rel": "details/20251214T034359Z/ollama_llama3_2.json"
      },
      {
        "model": "ollama:deepseek-r1:8b",
        "model_slug": "ollama_deepseek_r1_8b",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.0,
        "avg_wer": 22.411307842513683,
        "avg_cer": 23.864731719696557,
        "avg_token_sort_ratio": 1.142610080180663,
        "avg_chatter_ratio": 23.801864405482114,
        "avg_len_ratio": 24.801864405482114,
        "verbatim_count": 0,
        "verbatim_with_extras_count": 2,
        "inaccurate_count": 0,
        "hallucination_count": 8,
        "chattery_count": 10,
        "truncated_count": 0,
        "content_accuracy": 0.2,
        "grade": "F",
        "headline": "Low verbatim fidelity.",
        "notes": [
          "Frequently off-target (hallucinations).",
          "Often adds quotes/citations around correct text.",
          "Often adds extra chatter."
        ],
        "results_path": "runs/20251214T034359Z/ollama_deepseek_r1_8b/results.json",
        "details_path": "results/details/20251214T034359Z/ollama_deepseek_r1_8b.json",
        "details_rel": "details/20251214T034359Z/ollama_deepseek_r1_8b.json"
      }
    ]
  },
  {
    "run_id": "20251214T035411Z",
    "created_at": "2025-12-14T03:55:49.761802+00:00",
    "version": "kjv",
    "prompt_mode": "system2",
    "sample": {
      "count": 10,
      "seed": 1,
      "stratified": false
    },
    "models": [
      {
        "model": "reference:verbatim",
        "model_slug": "reference_verbatim",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 1.0,
        "avg_wer": 0.0,
        "avg_cer": 0.0,
        "avg_token_sort_ratio": 100.0,
        "avg_chatter_ratio": 0.0,
        "avg_len_ratio": 1.0,
        "verbatim_count": 10,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 0,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 0,
        "content_accuracy": 1.0,
        "grade": "A",
        "headline": "Excellent verbatim recall.",
        "notes": [
          "Behaves consistently on this sample."
        ],
        "results_path": "runs/20251214T035411Z/reference_verbatim/results.json",
        "details_path": "results/details/20251214T035411Z/reference_verbatim.json",
        "details_rel": "details/20251214T035411Z/reference_verbatim.json"
      },
      {
        "model": "ollama:llama3.2",
        "model_slug": "ollama_llama3_2",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.3,
        "avg_wer": 0.37810387765119224,
        "avg_cer": 0.27261782965270215,
        "avg_token_sort_ratio": 78.1494506631753,
        "avg_chatter_ratio": -0.1704044579533941,
        "avg_len_ratio": 0.8295955420466059,
        "verbatim_count": 3,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 7,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 4,
        "content_accuracy": 0.3,
        "grade": "D",
        "headline": "Mostly paraphrase/partial recall.",
        "notes": [
          "Often truncates output."
        ],
        "results_path": "runs/20251214T035411Z/ollama_llama3_2/results.json",
        "details_path": "results/details/20251214T035411Z/ollama_llama3_2.json",
        "details_rel": "details/20251214T035411Z/ollama_llama3_2.json"
      },
      {
        "model": "ollama:deepseek-r1:8b",
        "model_slug": "ollama_deepseek_r1_8b",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.2,
        "avg_wer": 0.5063176709328044,
        "avg_cer": 0.3783901131106203,
        "avg_token_sort_ratio": 76.45687618986412,
        "avg_chatter_ratio": -0.07303283648990286,
        "avg_len_ratio": 0.9269671635100971,
        "verbatim_count": 2,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 8,
        "hallucination_count": 0,
        "chattery_count": 1,
        "truncated_count": 4,
        "content_accuracy": 0.2,
        "grade": "F",
        "headline": "Low verbatim fidelity.",
        "notes": [
          "Often truncates output."
        ],
        "results_path": "runs/20251214T035411Z/ollama_deepseek_r1_8b/results.json",
        "details_path": "results/details/20251214T035411Z/ollama_deepseek_r1_8b.json",
        "details_rel": "details/20251214T035411Z/ollama_deepseek_r1_8b.json"
      }
    ]
  },
  {
    "run_id": "20251214T035832Z",
    "created_at": "2025-12-14T04:00:04.283096+00:00",
    "version": "kjv",
    "prompt_mode": "system2",
    "sample": {
      "count": 10,
      "seed": 1,
      "stratified": false
    },
    "models": [
      {
        "model": "reference:verbatim",
        "model_slug": "reference_verbatim",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 1.0,
        "avg_wer": 0.0,
        "avg_cer": 0.0,
        "avg_token_sort_ratio": 100.0,
        "avg_chatter_ratio": 0.0,
        "avg_len_ratio": 1.0,
        "verbatim_count": 10,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 0,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 0,
        "content_accuracy": 1.0,
        "clean_output_rate": 1.0,
        "strip_thinking_enabled": false,
        "strip_thinking_changed_count": 0,
        "raw_has_think_count": 0,
        "grade": "A",
        "headline": "Excellent verbatim recall.",
        "notes": [
          "Behaves consistently on this sample."
        ],
        "results_path": "runs/20251214T035832Z/reference_verbatim/results.json",
        "details_path": "results/details/20251214T035832Z/reference_verbatim.json",
        "details_rel": "details/20251214T035832Z/reference_verbatim.json"
      },
      {
        "model": "ollama:llama3.2",
        "model_slug": "ollama_llama3_2",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.3,
        "avg_wer": 0.37810387765119224,
        "avg_cer": 0.27261782965270215,
        "avg_token_sort_ratio": 78.1494506631753,
        "avg_chatter_ratio": -0.1704044579533941,
        "avg_len_ratio": 0.8295955420466059,
        "verbatim_count": 3,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 7,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 4,
        "content_accuracy": 0.3,
        "clean_output_rate": 1.0,
        "strip_thinking_enabled": false,
        "strip_thinking_changed_count": 0,
        "raw_has_think_count": 0,
        "grade": "D",
        "headline": "Mostly paraphrase/partial recall.",
        "notes": [
          "Often truncates output."
        ],
        "results_path": "runs/20251214T035832Z/ollama_llama3_2/results.json",
        "details_path": "results/details/20251214T035832Z/ollama_llama3_2.json",
        "details_rel": "details/20251214T035832Z/ollama_llama3_2.json"
      },
      {
        "model": "ollama:deepseek-r1:8b",
        "model_slug": "ollama_deepseek_r1_8b",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.2,
        "avg_wer": 0.5063176709328044,
        "avg_cer": 0.3783901131106203,
        "avg_token_sort_ratio": 76.45687618986412,
        "avg_chatter_ratio": -0.07303283648990286,
        "avg_len_ratio": 0.9269671635100971,
        "verbatim_count": 2,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 8,
        "hallucination_count": 0,
        "chattery_count": 1,
        "truncated_count": 4,
        "content_accuracy": 0.2,
        "clean_output_rate": 0.0,
        "strip_thinking_enabled": true,
        "strip_thinking_changed_count": 10,
        "raw_has_think_count": 10,
        "grade": "F",
        "headline": "Low verbatim fidelity.",
        "notes": [
          "Often truncates output.",
          "Contains visible thinking; hidden for scoring."
        ],
        "results_path": "runs/20251214T035832Z/ollama_deepseek_r1_8b/results.json",
        "details_path": "results/details/20251214T035832Z/ollama_deepseek_r1_8b.json",
        "details_rel": "details/20251214T035832Z/ollama_deepseek_r1_8b.json"
      }
    ]
  },
  {
    "run_id": "20251214T041032Z",
    "created_at": "2025-12-14T04:12:19.992747+00:00",
    "version": "kjv",
    "prompt_mode": "system2",
    "sample": {
      "count": 10,
      "seed": 1,
      "stratified": false
    },
    "models": [
      {
        "model": "reference:verbatim",
        "model_slug": "reference_verbatim",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 1.0,
        "avg_wer": 0.0,
        "avg_cer": 0.0,
        "avg_token_sort_ratio": 100.0,
        "avg_chatter_ratio": 0.0,
        "avg_len_ratio": 1.0,
        "verbatim_count": 10,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 0,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 0,
        "content_accuracy": 1.0,
        "clean_output_rate": 1.0,
        "strip_thinking_enabled": false,
        "strip_thinking_changed_count": 0,
        "raw_has_think_count": 0,
        "grade": "A",
        "headline": "Excellent verbatim recall.",
        "notes": [
          "Behaves consistently on this sample."
        ],
        "results_path": "runs/20251214T041032Z/reference_verbatim/results.json",
        "details_path": "results/details/20251214T041032Z/reference_verbatim.json",
        "details_rel": "details/20251214T041032Z/reference_verbatim.json"
      },
      {
        "model": "ollama:llama3.2",
        "model_slug": "ollama_llama3_2",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.3,
        "avg_wer": 0.37810387765119224,
        "avg_cer": 0.27261782965270215,
        "avg_token_sort_ratio": 78.1494506631753,
        "avg_chatter_ratio": -0.1704044579533941,
        "avg_len_ratio": 0.8295955420466059,
        "verbatim_count": 3,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 7,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 4,
        "content_accuracy": 0.3,
        "clean_output_rate": 1.0,
        "strip_thinking_enabled": false,
        "strip_thinking_changed_count": 0,
        "raw_has_think_count": 0,
        "grade": "D",
        "headline": "Mostly paraphrase/partial recall.",
        "notes": [
          "Often truncates output."
        ],
        "results_path": "runs/20251214T041032Z/ollama_llama3_2/results.json",
        "details_path": "results/details/20251214T041032Z/ollama_llama3_2.json",
        "details_rel": "details/20251214T041032Z/ollama_llama3_2.json"
      },
      {
        "model": "ollama:deepseek-r1:8b",
        "model_slug": "ollama_deepseek_r1_8b",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.2,
        "avg_wer": 0.5063176709328044,
        "avg_cer": 0.3783901131106203,
        "avg_token_sort_ratio": 76.45687618986412,
        "avg_chatter_ratio": -0.07303283648990286,
        "avg_len_ratio": 0.9269671635100971,
        "verbatim_count": 2,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 8,
        "hallucination_count": 0,
        "chattery_count": 1,
        "truncated_count": 4,
        "content_accuracy": 0.2,
        "clean_output_rate": 0.0,
        "strip_thinking_enabled": true,
        "strip_thinking_changed_count": 10,
        "raw_has_think_count": 10,
        "grade": "F",
        "headline": "Low verbatim fidelity.",
        "notes": [
          "Often truncates output.",
          "Contains visible thinking; hidden for scoring."
        ],
        "results_path": "runs/20251214T041032Z/ollama_deepseek_r1_8b/results.json",
        "details_path": "results/details/20251214T041032Z/ollama_deepseek_r1_8b.json",
        "details_rel": "details/20251214T041032Z/ollama_deepseek_r1_8b.json"
      },
      {
        "model": "ollama:gemma3:4b",
        "model_slug": "ollama_gemma3_4b",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.3,
        "avg_wer": 0.667289951281044,
        "avg_cer": 0.5440328946031774,
        "avg_token_sort_ratio": 65.95313296279758,
        "avg_chatter_ratio": -0.019131151755008412,
        "avg_len_ratio": 0.9808688482449915,
        "verbatim_count": 3,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 7,
        "hallucination_count": 0,
        "chattery_count": 1,
        "truncated_count": 5,
        "content_accuracy": 0.3,
        "clean_output_rate": 1.0,
        "strip_thinking_enabled": false,
        "strip_thinking_changed_count": 0,
        "raw_has_think_count": 0,
        "grade": "D",
        "headline": "Mostly paraphrase/partial recall.",
        "notes": [
          "Often truncates output."
        ],
        "results_path": "runs/20251214T041032Z/ollama_gemma3_4b/results.json",
        "details_path": "results/details/20251214T041032Z/ollama_gemma3_4b.json",
        "details_rel": "details/20251214T041032Z/ollama_gemma3_4b.json"
      }
    ]
  },
  {
    "run_id": "20251214T042308Z",
    "created_at": "2025-12-14T04:25:31.359367+00:00",
    "version": "kjv",
    "prompt_mode": "system2",
    "sample": {
      "count": 10,
      "seed": 1,
      "stratified": false
    },
    "models": [
      {
        "model": "reference:verbatim",
        "model_slug": "reference_verbatim",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 1.0,
        "avg_wer": 0.0,
        "avg_cer": 0.0,
        "avg_token_sort_ratio": 100.0,
        "avg_chatter_ratio": 0.0,
        "avg_len_ratio": 1.0,
        "verbatim_count": 10,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 0,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 0,
        "content_accuracy": 1.0,
        "clean_output_rate": 1.0,
        "strip_thinking_enabled": false,
        "strip_thinking_changed_count": 0,
        "raw_has_think_count": 0,
        "grade": "A",
        "headline": "Excellent verbatim recall.",
        "notes": [
          "Behaves consistently on this sample."
        ],
        "results_path": "runs/20251214T042308Z/reference_verbatim/results.json",
        "details_path": "results/details/20251214T042308Z/reference_verbatim.json",
        "details_rel": "details/20251214T042308Z/reference_verbatim.json"
      },
      {
        "model": "ollama:llama3.2",
        "model_slug": "ollama_llama3_2",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.3,
        "avg_wer": 0.37810387765119224,
        "avg_cer": 0.27261782965270215,
        "avg_token_sort_ratio": 78.1494506631753,
        "avg_chatter_ratio": -0.1704044579533941,
        "avg_len_ratio": 0.8295955420466059,
        "verbatim_count": 3,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 7,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 4,
        "content_accuracy": 0.3,
        "clean_output_rate": 1.0,
        "strip_thinking_enabled": false,
        "strip_thinking_changed_count": 0,
        "raw_has_think_count": 0,
        "grade": "D",
        "headline": "Mostly paraphrase/partial recall.",
        "notes": [
          "Often truncates output."
        ],
        "results_path": "runs/20251214T042308Z/ollama_llama3_2/results.json",
        "details_path": "results/details/20251214T042308Z/ollama_llama3_2.json",
        "details_rel": "details/20251214T042308Z/ollama_llama3_2.json"
      },
      {
        "model": "ollama:deepseek-r1:8b",
        "model_slug": "ollama_deepseek_r1_8b",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.2,
        "avg_wer": 0.5063176709328044,
        "avg_cer": 0.3783901131106203,
        "avg_token_sort_ratio": 76.45687618986412,
        "avg_chatter_ratio": -0.07303283648990286,
        "avg_len_ratio": 0.9269671635100971,
        "verbatim_count": 2,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 8,
        "hallucination_count": 0,
        "chattery_count": 1,
        "truncated_count": 4,
        "content_accuracy": 0.2,
        "clean_output_rate": 0.0,
        "strip_thinking_enabled": true,
        "strip_thinking_changed_count": 10,
        "raw_has_think_count": 10,
        "grade": "F",
        "headline": "Low verbatim fidelity.",
        "notes": [
          "Often truncates output.",
          "Contains visible thinking; hidden for scoring."
        ],
        "results_path": "runs/20251214T042308Z/ollama_deepseek_r1_8b/results.json",
        "details_path": "results/details/20251214T042308Z/ollama_deepseek_r1_8b.json",
        "details_rel": "details/20251214T042308Z/ollama_deepseek_r1_8b.json"
      },
      {
        "model": "ollama:gemma3:4b",
        "model_slug": "ollama_gemma3_4b",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.3,
        "avg_wer": 0.667289951281044,
        "avg_cer": 0.5440328946031774,
        "avg_token_sort_ratio": 65.95313296279758,
        "avg_chatter_ratio": -0.019131151755008412,
        "avg_len_ratio": 0.9808688482449915,
        "verbatim_count": 3,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 7,
        "hallucination_count": 0,
        "chattery_count": 1,
        "truncated_count": 5,
        "content_accuracy": 0.3,
        "clean_output_rate": 1.0,
        "strip_thinking_enabled": false,
        "strip_thinking_changed_count": 0,
        "raw_has_think_count": 0,
        "grade": "D",
        "headline": "Mostly paraphrase/partial recall.",
        "notes": [
          "Often truncates output."
        ],
        "results_path": "runs/20251214T042308Z/ollama_gemma3_4b/results.json",
        "details_path": "results/details/20251214T042308Z/ollama_gemma3_4b.json",
        "details_rel": "details/20251214T042308Z/ollama_gemma3_4b.json"
      },
      {
        "model": "ollama:qwen3",
        "model_slug": "ollama_qwen3",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.4,
        "avg_wer": 0.4632850241545893,
        "avg_cer": 0.45194285714285715,
        "avg_token_sort_ratio": 57.10659898477157,
        "avg_chatter_ratio": -0.4424,
        "avg_len_ratio": 0.5576,
        "verbatim_count": 4,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 2,
        "hallucination_count": 4,
        "chattery_count": 0,
        "truncated_count": 5,
        "content_accuracy": 0.4,
        "clean_output_rate": 1.0,
        "strip_thinking_enabled": false,
        "strip_thinking_changed_count": 0,
        "raw_has_think_count": 0,
        "grade": "D",
        "headline": "Mostly paraphrase/partial recall.",
        "notes": [
          "Frequently off-target (hallucinations).",
          "Often truncates output."
        ],
        "results_path": "runs/20251214T042308Z/ollama_qwen3/results.json",
        "details_path": "results/details/20251214T042308Z/ollama_qwen3.json",
        "details_rel": "details/20251214T042308Z/ollama_qwen3.json"
      }
    ]
  },
  {
    "run_id": "20251214T043408Z",
    "created_at": "2025-12-14T04:36:00.071881+00:00",
    "version": "kjv",
    "prompt_mode": "system2",
    "sample": {
      "count": 10,
      "seed": 1,
      "stratified": false
    },
    "models": [
      {
        "model": "reference:verbatim",
        "model_slug": "reference_verbatim",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 1.0,
        "avg_wer": 0.0,
        "avg_cer": 0.0,
        "avg_token_sort_ratio": 100.0,
        "avg_chatter_ratio": 0.0,
        "avg_len_ratio": 1.0,
        "verbatim_count": 10,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 0,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 0,
        "content_accuracy": 1.0,
        "clean_output_rate": 1.0,
        "strip_thinking_enabled": false,
        "strip_thinking_changed_count": 0,
        "raw_has_think_count": 0,
        "grade": "A",
        "headline": "Excellent verbatim recall.",
        "notes": [
          "Behaves consistently on this sample."
        ],
        "results_path": "runs/20251214T043408Z/reference_verbatim/results.json",
        "details_path": "results/details/20251214T043408Z/reference_verbatim.json",
        "details_rel": "details/20251214T043408Z/reference_verbatim.json"
      },
      {
        "model": "ollama:llama3.2",
        "model_slug": "ollama_llama3_2",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.3,
        "avg_wer": 0.37810387765119224,
        "avg_cer": 0.27261782965270215,
        "avg_token_sort_ratio": 78.1494506631753,
        "avg_chatter_ratio": -0.1704044579533941,
        "avg_len_ratio": 0.8295955420466059,
        "verbatim_count": 3,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 7,
        "hallucination_count": 0,
        "chattery_count": 0,
        "truncated_count": 4,
        "content_accuracy": 0.3,
        "clean_output_rate": 1.0,
        "strip_thinking_enabled": false,
        "strip_thinking_changed_count": 0,
        "raw_has_think_count": 0,
        "grade": "D",
        "headline": "Mostly paraphrase/partial recall.",
        "notes": [
          "Often truncates output."
        ],
        "results_path": "runs/20251214T043408Z/ollama_llama3_2/results.json",
        "details_path": "results/details/20251214T043408Z/ollama_llama3_2.json",
        "details_rel": "details/20251214T043408Z/ollama_llama3_2.json"
      },
      {
        "model": "ollama:deepseek-r1:8b",
        "model_slug": "ollama_deepseek_r1_8b",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.2,
        "avg_wer": 0.5063176709328044,
        "avg_cer": 0.3783901131106203,
        "avg_token_sort_ratio": 76.45687618986412,
        "avg_chatter_ratio": -0.07303283648990286,
        "avg_len_ratio": 0.9269671635100971,
        "verbatim_count": 2,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 8,
        "hallucination_count": 0,
        "chattery_count": 1,
        "truncated_count": 4,
        "content_accuracy": 0.2,
        "clean_output_rate": 0.0,
        "strip_thinking_enabled": true,
        "strip_thinking_changed_count": 10,
        "raw_has_think_count": 10,
        "grade": "F",
        "headline": "Low verbatim fidelity.",
        "notes": [
          "Often truncates output.",
          "Contains visible thinking; hidden for scoring."
        ],
        "results_path": "runs/20251214T043408Z/ollama_deepseek_r1_8b/results.json",
        "details_path": "results/details/20251214T043408Z/ollama_deepseek_r1_8b.json",
        "details_rel": "details/20251214T043408Z/ollama_deepseek_r1_8b.json"
      },
      {
        "model": "ollama:gemma3:4b",
        "model_slug": "ollama_gemma3_4b",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.3,
        "avg_wer": 0.667289951281044,
        "avg_cer": 0.5440328946031774,
        "avg_token_sort_ratio": 65.95313296279758,
        "avg_chatter_ratio": -0.019131151755008412,
        "avg_len_ratio": 0.9808688482449915,
        "verbatim_count": 3,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 7,
        "hallucination_count": 0,
        "chattery_count": 1,
        "truncated_count": 5,
        "content_accuracy": 0.3,
        "clean_output_rate": 1.0,
        "strip_thinking_enabled": false,
        "strip_thinking_changed_count": 0,
        "raw_has_think_count": 0,
        "grade": "D",
        "headline": "Mostly paraphrase/partial recall.",
        "notes": [
          "Often truncates output."
        ],
        "results_path": "runs/20251214T043408Z/ollama_gemma3_4b/results.json",
        "details_path": "results/details/20251214T043408Z/ollama_gemma3_4b.json",
        "details_rel": "details/20251214T043408Z/ollama_gemma3_4b.json"
      },
      {
        "model": "ollama:qwen3",
        "model_slug": "ollama_qwen3",
        "version": "kjv",
        "prompt_mode": "system2",
        "n": 10,
        "strict_accuracy": 0.3,
        "avg_wer": 0.8052180629827689,
        "avg_cer": 0.7200544428068169,
        "avg_token_sort_ratio": 69.11794761051894,
        "avg_chatter_ratio": 0.22811398080976927,
        "avg_len_ratio": 1.2281139808097692,
        "verbatim_count": 3,
        "verbatim_with_extras_count": 0,
        "inaccurate_count": 5,
        "hallucination_count": 2,
        "chattery_count": 1,
        "truncated_count": 3,
        "content_accuracy": 0.3,
        "clean_output_rate": 1.0,
        "strip_thinking_enabled": false,
        "strip_thinking_changed_count": 0,
        "raw_has_think_count": 0,
        "grade": "D",
        "headline": "Mostly paraphrase/partial recall.",
        "notes": [
          "Some off-target responses.",
          "Often truncates output."
        ],
        "results_path": "runs/20251214T043408Z/ollama_qwen3/results.json",
        "details_path": "results/details/20251214T043408Z/ollama_qwen3.json",
        "details_rel": "details/20251214T043408Z/ollama_qwen3.json"
      }
    ]
  }
]